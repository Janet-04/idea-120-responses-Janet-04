#Bridle Response

James Bridle's concerns seem comprehensible enough to me, only at times it can feel overwhelming and exaggerated. However, I can see why it is concerning that there are many knock-off versions of cartoon characters on YouTube that display inappropriate scenarios. I remember seeing not that long ago in the news of a frightening, long-faced, dark-haired woman (not a real person), who was used to pop-up randomly between child-friendly videos. In between those pop-ups, there was also audio encouraging kids to commit suicide. The latter, along with Bridle's examples, are completely understandable instances as to why it is so concerning that children are being exposed to such images and audios. This form of exposing children to inappropriate content is unfortunately effective because, like in the example I provided, thumbnails and titles can be misleading and be believed to be appropriate. Parents often find a compilation of videos and have their children watch it continuously without being aware what the video actually contains throughout it. In other words, children watch these compilations fully from beginning to end, while parents don't. Therefore, it is hard for parents to notice when a video contains inappropriate content because they aren't actively watching an hour long cartoon video, like their kids.

Furthermore, Bridle's acknowledgement of automated content seems fairly accurate as well. The fact that many child-oriented channels can possibly be controlled by bots, is not surprising. It is known for YouTube to have bots commenting, subscribing, and liking. Also, having automated content is not favorable because the room for error is widely open without having humans manually control it and approve it. In other words, since content is automated, violent and other unfavorable scenarios can be created by bots. As Bridle says, platforms like YouTube might have to delete their whole system in order to stop all these issues from happening, but it is a highly unlikely thing to happen. I am sure there has to be another way to solve this issue. I can see Bridle's concern with automatization and algorithms as well because I do know YouTube has had many complaints from other content creators regarding its algorithm. Often, certain channels are not advertised, recommended, or their notification bell does not work for their subscribers. This is all circling back to Bridle's critique of YouTube's algorithm. Lastly, Bridle's claim that, "the system is complicit in the abuse" (Bridle, 2017) seems plausible because YouTube does have the power to do anything to its platform, but they aren't being as reactive as they should be. 
